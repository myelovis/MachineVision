{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913544e",
   "metadata": {},
   "source": [
    "| Task| Student | Teacher | Date| \n",
    "|----------|----------|----------|--------|\n",
    "|Pseudo Object Detection Code Correction| Lovisa Nguyen | Toni Aaltonen |11/23|"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce8544cf",
   "metadata": {},
   "source": [
    "In class we looked this \"object detection\" example, we noticed that is was not really a object detection.\n",
    "Example at: https://circuitdigest.com/tutorial/real-life-object-detection-using-opencv-python-detecting-objects-in-live-video\n",
    "Now your task is to take the code, and modify it to really work as object detection with ORB.\n",
    "You can use sliding window with ORB, or check the cluster of detected points. Draw bounding box around the detection.\n",
    "Return the jupyter notebook, and 2 min video, that explains your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a3f8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def BLUR(image):\n",
    "    # Transfrom image to its gray-version for a better highlight of edges\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Blurring Filter\n",
    "    # Scanning using kernel_blurring by taking (average of each pixel)/9 with its neigbour ones\n",
    "    kernel_blurring = np.array([\n",
    "                                [1,1,1],\n",
    "                                [1,1,1],\n",
    "                                [1,1,1], \n",
    "                               ])/9 \n",
    "    image_blurred = cv2.filter2D(image_gray, -1, kernel_blurring)\n",
    "    # Edge Detection via Gradient\n",
    "    image_gray_float = image_blurred #.astype(np.float32)\n",
    "    return image_gray_float\n",
    "\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    edges0 = cv2.Canny(new_image,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    edges1 = cv2.Canny(image_template,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(edges0, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(edges1, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches\n",
    "    #img2 = cv2.drawKeypoints(edges1, kp2, None, color=(0,255,0), flags=0)\n",
    "    #img1 = cv2.drawKeypoints(edges0, kp1, None, color=(0,255,0), flags=0)\n",
    "    #plt.imshow(img1)\n",
    "    #plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "# Load video file and template image\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "image_template = cv2.imread('HisPelivideo.png', 0)\n",
    "\n",
    "\n",
    "# Initialize ROI coordinates\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = 0, 0, 0, 0\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    # Get video frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the video frame was captured successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for ORB detector\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get number of ORB matches\n",
    "    matches = ORB_detector(gray_frame, image_template)\n",
    "\n",
    "    # Display status string showing the current number of matches\n",
    "    output_string = \"Matches = \" + str(matches)\n",
    "    cv2.putText(frame, output_string, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Our threshold to indicate object detection\n",
    "    threshold = 337\n",
    "\n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        # Perform template matching to get the location of the detected object\n",
    "        result = cv2.matchTemplate(gray_frame, image_template, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        top_left_x, top_left_y = max_loc\n",
    "        bottom_right_x = top_left_x + image_template.shape[1]\n",
    "        bottom_right_y = top_left_y + image_template.shape[0]\n",
    "        \n",
    "        # Resize the frame region to match the detected object's size\n",
    "        detected_object_width = bottom_right_x - top_left_x\n",
    "        detected_object_height = bottom_right_y - top_left_x\n",
    "        # Update the top-left and bottom-right coordinates of the detection frame\n",
    "        detection_frame_width = detected_object_width\n",
    "        detection_frame_height = detected_object_height\n",
    "        # Draw a rectangle around the detected object with the resized frame size\n",
    "        cv2.rectangle(frame, (int(top_left_x), int(top_left_y)),\n",
    "                      (int(top_left_x + detection_frame_width), int(top_left_y + detection_frame_height)),\n",
    "                      (0, 255, 0), 3)\n",
    "        cv2.putText(frame, 'Object Found', (50, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with the detected object\n",
    "    cv2.imshow('Object Detector using ORB', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c0e1f06",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_813/4153953804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Draw the detecting frame around the object using the matched keypoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryIdx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "### Main\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    edges0 = cv2.Canny(new_image,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    edges1 = cv2.Canny(image_template,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(edges0, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(edges1, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches\n",
    "    #img2 = cv2.drawKeypoints(edges1, kp2, None, color=(0,255,0), flags=0)\n",
    "    #img1 = cv2.drawKeypoints(edges0, kp1, None, color=(0,255,0), flags=0)\n",
    "    #plt.imshow(img1)\n",
    "    #plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_time = 3\n",
    "init_frame = int(target_time * fps)\n",
    "ret, initial_frame = cap.read()\n",
    "gray_initial_frame = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize ROI coordinates\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Get webcam images\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Get height and width of webcam frame\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Define ROI Box Dimensions (Note some of these things should be outside the loop)\n",
    "    top_left_x = int(width / 3)\n",
    "    top_left_y = int((height / 2) + (height / 4))\n",
    "    bottom_right_x = int((width / 3) * 2)\n",
    "    bottom_right_y = int((height / 2) - (height / 4))\n",
    "\n",
    "    # Draw rectangular window for our region of interest\n",
    "    # cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "\n",
    "    # Crop window of observation we defined above\n",
    "    cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "    # Flip frame orientation horizontally\n",
    "    frame = cv2.flip(frame,1)\n",
    "\n",
    "    # Get number of ORB matches \n",
    "    matches = ORB_detector(cropped, image_template)\n",
    "\n",
    "    # Display status string showing the current no. of matches \n",
    "    output_string = \"Matches = \" + str(matches)\n",
    "    cv2.putText(frame, output_string, (50,450), cv2.FONT_HERSHEY_COMPLEX, 2, (250,0,150), 2)\n",
    "\n",
    "    # Our threshold to indicate object deteciton\n",
    "    # For new images or lightening conditions you may need to experiment a bit \n",
    "    # Note: The ORB detector to get the top 1000 matches, 350 is essentially a min 35% match\n",
    "    threshold = 336\n",
    "\n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        # Update the initial frame and keypoints for the next iteration\n",
    "        initial_frame = frame\n",
    "        kp1 = kp2\n",
    "\n",
    "        # Draw the detecting frame around the object using the matched keypoints\n",
    "        # Example: \n",
    "        for match in matches[:10]:\n",
    "            x, y = kp1[match.queryIdx].pt\n",
    "            cv2.rectangle(frame, (int(x + 10), int(y + 10)), (int(x + 100), int(y + 100)), (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), (0,255,0), 3)\n",
    "        cv2.putText(frame,'Object Found',(50,50), cv2.FONT_HERSHEY_COMPLEX, 2 ,(0,255,0), 2)\n",
    "        # Display the frame with the detected object\n",
    "        cv2.imshow('Object Detector using ORB', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96705951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background data type: uint8\n",
      "Background data type after conversion: uint8\n",
      "Background data type: uint8\n",
      "Background data type: uint8\n",
      "Background shape: (360, 640, 3)\n",
      "Gray shape: (360, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_background(file_path):\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Handle frames with different dimensions, if necessary\n",
    "        # frame = cv2.resize(frame, (desired_width, desired_height))\n",
    "        frames.append(frame)\n",
    "    cap.release()  # Release the video capture object\n",
    "    if len(frames) > 0:\n",
    "        median_frame = np.median(frames, axis=0).astype(np.uint8)\n",
    "        return median_frame\n",
    "    else:\n",
    "        # Handle the case when no valid frames are read\n",
    "        print(\"Error: No valid frames were read from the video.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "background = get_background(\"video_1.mp4\")\n",
    "\n",
    "if background is not None:\n",
    "    print(\"Background data type:\", background.dtype)\n",
    "    background_gray = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "    print(\"Background data type after conversion:\", background_gray.dtype)\n",
    "else:\n",
    "    print(\"Error: No valid background frame.\")\n",
    "\n",
    "\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    orb = cv2.ORB_create(1000, 1.2, 2, 31)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(new_image, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(image_template, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda val: val.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(image_template, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(new_image, kp2, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "# Load video file and template image\n",
    "\n",
    "consecutive_frames = 500\n",
    "\n",
    "background = get_background(\"video_1.mp4\")\n",
    "print(\"Background data type:\", background.dtype)\n",
    "background_gray = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "print(\"Background data type:\", background.dtype)\n",
    "\n",
    "\n",
    "frame_count = 0\n",
    "frame_diff_list = []\n",
    "\n",
    "while True:\n",
    "    cap = cv2.VideoCapture(\"video_1.mp4\")\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frame_count += 1\n",
    "    orig_frame = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.uint8)\n",
    "    print(\"Background shape:\", background.shape)\n",
    "    print(\"Gray shape:\", gray.shape)\n",
    "\n",
    "    frame_diff = cv2.absdiff(gray, background_gray)\n",
    "    ret, thres = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "    dilate_frame = cv2.dilate(thres, None, iterations=2)\n",
    "    frame_diff_list.append(dilate_frame)\n",
    "    if len(frame_diff_list) == consecutive_frames:\n",
    "        sum_frames = sum(frame_diff_list)\n",
    "        contours, hierarchy = cv2.findContours(sum_frames, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 500:\n",
    "                continue\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(orig_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Get height and width of webcam frame\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Define ROI Box Dimensions (Note some of these things should be outside the loop)\n",
    "        top_left_x = int(width / 3)\n",
    "        top_left_y = int((height / 2) + (height / 4))\n",
    "        bottom_right_x = int((width / 3) * 2)\n",
    "        bottom_right_y = int((height / 2) - (height / 4))\n",
    "\n",
    "        # Draw rectangular window for our region of interest\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "\n",
    "        # Crop window of observation we defined above\n",
    "        cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "        # Flip frame orientation horizontally\n",
    "        frame = cv2.flip(frame,1)\n",
    "\n",
    "        # Get number of ORB matches\n",
    "        matches = ORB_detector(cropped, image_template)\n",
    "\n",
    "        # Display status string showing the current no. of matches\n",
    "        output_string = \"Matches = \" + str(matches)\n",
    "        cv2.putText(frame, output_string, (50,450), cv2.FONT_HERSHEY_COMPLEX, 2, (250,0,150), 2)\n",
    "\n",
    "        # Our threshold to indicate object deteciton\n",
    "        # For new images or lightening conditions you may need to experiment a bit\n",
    "        # Note: The ORB detector to get the top 1000 matches, 350 is essentially a min 35% match\n",
    "        threshold = 336\n",
    "\n",
    "        # If matches exceed our threshold then object has been detected\n",
    "        if matches > threshold:\n",
    "            cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), (0,255,0), 3)\n",
    "            cv2.putText(frame,'Object Found',(50,50), cv2.FONT_HERSHEY_COMPLEX, 2 ,(0,255,0), 2)\n",
    "            # Display the frame with the detected object\n",
    "        cv2.imshow('Object Detector using ORB', frame)\n",
    "\n",
    "        cv2.imshow('Detected Objects', orig_frame)\n",
    "        if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d6d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on this, deeper into the ORB\n",
    "\n",
    "import cv2\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    orb = cv2.ORB_create(1000, 1.2, 2, 31)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(new_image, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(image_template, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda val: val.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(image_template, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(new_image, kp2, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "target_time = 3\n",
    "init_frame = int(target_time * fps)\n",
    "ret, initial_frame = cap.read()\n",
    "gray_initial_frame = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2GRAY)\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    \n",
    "    orb = cv2.ORB_create()\n",
    "    kp1, des1 = orb.detectAndCompute(gray_initial_frame, None)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect and compute ORB keypoints and descriptors for the current frame\n",
    "    kp2, des2 = orb.detectAndCompute(frame, None)\n",
    "\n",
    "    # Match descriptors using the BFMatcher\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    \n",
    "    # Draw matches on the frames (optional)\n",
    "    # img_matches = cv2.drawMatches(initial_frame, kp1, frame, kp2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Update the initial frame and keypoints for the next iteration\n",
    "    initial_frame = frame\n",
    "    kp1 = kp2\n",
    "    \n",
    "    threshold = 341\n",
    "    # Draw the detecting frame around the object using the matched keypoints\n",
    "    # Example: \n",
    "    \n",
    "    for match in matches[:50]:\n",
    "        x, y = kp1[match.queryIdx].pt\n",
    "        cv2.rectangle(frame, (int(x+20), int(y+20)), (int(x+10), int(y+10)), (0, 255, 0), 2)\n",
    "        \n",
    "        \n",
    "        # Define the center, axes lengths, angle, and color for the ellipse\n",
    "        center = (int(x), int(y))  # (x, y) coordinates of the center\n",
    "        axes_lengths = (50, 20)  # Lengths of major and minor axes (adjust as needed)\n",
    "        angle = 0  # Rotation angle in degrees\n",
    "        start_angle = 0  # Start angle of the ellipse arc in degrees\n",
    "        end_angle = 360  # End angle of the ellipse arc in degrees\n",
    "        color = (0, 255, 0)  # Green color in BGR\n",
    "\n",
    "        # Define the thickness of the ellipse outline (-1 for filled ellipse)\n",
    "        thickness = 2\n",
    "\n",
    "        # Draw the ellipse around the point (x, y)\n",
    "        cv2.ellipse(frame, center, axes_lengths, angle, start_angle, end_angle, color, thickness)\n",
    "\n",
    "        # Draw the smaller rectangle around the point (x, y)\n",
    "        cv2.rectangle(frame, (int(x + 20), int(y + 20)), (int(x + 10), int(y + 10)), (0, 255, 0), 2)\n",
    "\n",
    "        # Show the image with the ellipse and rectangle\n",
    "        cv2.imshow(\"Ellipse and Rectangle\", frame)\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
