{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b346c187",
   "metadata": {},
   "source": [
    "| Task| Student | Teacher | Date| \n",
    "|----------|----------|----------|--------|\n",
    "|Pseudo Object Detection Code Correction| Lovisa Nguyen | Toni Aaltonen |11/23|"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4d105ec",
   "metadata": {},
   "source": [
    "In class we looked this \"object detection\" example, we noticed that is was not really a object detection.\n",
    "Example at: https://circuitdigest.com/tutorial/real-life-object-detection-using-opencv-python-detecting-objects-in-live-video\n",
    "Now your task is to take the code, and modify it to really work as object detection with ORB.\n",
    "You can use sliding window with ORB, or check the cluster of detected points. Draw bounding box around the detection.\n",
    "Return the jupyter notebook, and 2 min video, that explains your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50947d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def BLUR(image):\n",
    "    # Transfrom image to its gray-version for a better highlight of edges\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Blurring Filter\n",
    "    # Scanning using kernel_blurring by taking (average of each pixel)/9 with its neigbour ones\n",
    "    kernel_blurring = np.array([\n",
    "                                [1,1,1],\n",
    "                                [1,1,1],\n",
    "                                [1,1,1], \n",
    "                               ])/9 \n",
    "    image_blurred = cv2.filter2D(image_gray, -1, kernel_blurring)\n",
    "    # Edge Detection via Gradient\n",
    "    image_gray_float = image_blurred #.astype(np.float32)\n",
    "    return image_gray_float\n",
    "\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    edges0 = cv2.Canny(new_image,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    edges1 = cv2.Canny(image_template,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(edges0, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(edges1, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(edges1, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(edges0, kp1, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img1)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "# Load video file and template image\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "image_template = cv2.imread('HisPelivideo.png', 0)\n",
    "\n",
    "\n",
    "# Initialize ROI coordinates\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = 0, 0, 0, 0\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    # Get video frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the video frame was captured successfully\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for ORB detector\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get number of ORB matches\n",
    "    matches = ORB_detector(gray_frame, image_template)\n",
    "\n",
    "    # Display status string showing the current number of matches\n",
    "    output_string = \"Matches = \" + str(matches)\n",
    "    cv2.putText(frame, output_string, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Our threshold to indicate object detection\n",
    "    threshold = 337\n",
    "\n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        # Perform template matching to get the location of the detected object\n",
    "        result = cv2.matchTemplate(gray_frame, image_template, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
    "        top_left_x, top_left_y = max_loc\n",
    "        bottom_right_x = top_left_x + image_template.shape[1]\n",
    "        bottom_right_y = top_left_y + image_template.shape[0]\n",
    "        \n",
    "        # Resize the frame region to match the detected object's size\n",
    "        detected_object_width = bottom_right_x - top_left_x\n",
    "        detected_object_height = bottom_right_y - top_left_x\n",
    "        # Update the top-left and bottom-right coordinates of the detection frame\n",
    "        detection_frame_width = detected_object_width\n",
    "        detection_frame_height = detected_object_height\n",
    "        # Draw a rectangle around the detected object with the resized frame size\n",
    "        cv2.rectangle(frame, (int(top_left_x), int(top_left_y)),\n",
    "                      (int(top_left_x + detection_frame_width), int(top_left_y + detection_frame_height)),\n",
    "                      (0, 255, 0), 3)\n",
    "        cv2.putText(frame, 'Object Found', (50, 100), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with the detected object\n",
    "    cv2.imshow('Object Detector using ORB', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    edges0 = cv2.Canny(new_image,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    edges1 = cv2.Canny(image_template,10,50, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(edges0, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(edges1, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(edges1, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(edges0, kp1, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img1)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "# Load video file and template image\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "image_template = cv2.imread('Pelivideo.png', 0)\n",
    "\n",
    "\n",
    "# Initialize ROI coordinates\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Get webcam images\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Get height and width of webcam frame\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Define ROI Box Dimensions (Note some of these things should be outside the loop)\n",
    "    top_left_x = int(width / 3)\n",
    "    top_left_y = int((height / 2) + (height / 4))\n",
    "    bottom_right_x = int((width / 3) * 2)\n",
    "    bottom_right_y = int((height / 2) - (height / 4))\n",
    "\n",
    "    # Draw rectangular window for our region of interest\n",
    "    # cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "\n",
    "    # Crop window of observation we defined above\n",
    "    cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "    # Flip frame orientation horizontally\n",
    "    frame = cv2.flip(frame,1)\n",
    "\n",
    "    # Get number of ORB matches \n",
    "    matches = ORB_detector(cropped, image_template)\n",
    "\n",
    "    # Display status string showing the current no. of matches \n",
    "    output_string = \"Matches = \" + str(matches)\n",
    "    cv2.putText(frame, output_string, (50,450), cv2.FONT_HERSHEY_COMPLEX, 2, (250,0,150), 2)\n",
    "\n",
    "    # Our threshold to indicate object deteciton\n",
    "    # For new images or lightening conditions you may need to experiment a bit \n",
    "    # Note: The ORB detector to get the top 1000 matches, 350 is essentially a min 35% match\n",
    "    threshold = 336\n",
    "\n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), (0,255,0), 3)\n",
    "        cv2.putText(frame,'Object Found',(50,50), cv2.FONT_HERSHEY_COMPLEX, 2 ,(0,255,0), 2)\n",
    "        # Display the frame with the detected object\n",
    "        cv2.imshow('Object Detector using ORB', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0de1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -i INPUT [-c CONSECUTIVE_FRAMES]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -i/--input\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(new_image, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(image_template, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda val: val.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(image_template, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(new_image, kp2, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-i', '--input', help='path to the input video',\n",
    "                    required=True)\n",
    "parser.add_argument('-c', '--consecutive-frames', default=4, type=int,\n",
    "                    dest='consecutive_frames', help='path to the input video')\n",
    "args = vars(parser.parse_args())\n",
    "frame_count = 0\n",
    "consecutive_frame = args['consecutive_frames']\n",
    "# Load video file and template image\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "image_template = cv2.imread('Pelivideo.png', 0)\n",
    "\n",
    "# Initialize ROI coordinates\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = 0, 0, 0, 0\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    frame_count = 0\n",
    "    consecutive_frame = args['consecutive_frames']\n",
    "    # Get video frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        frame_count += 1\n",
    "        orig_frame = frame.copy()\n",
    "        # IMPORTANT STEP: convert the frame to grayscale first\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if frame_count % consecutive_frame == 0 or frame_count == 1:\n",
    "            frame_diff_list = []\n",
    "        # find the difference between current frame and base frame\n",
    "        frame_diff = cv2.absdiff(gray, background)\n",
    "        # thresholding to convert the frame to binary\n",
    "        ret, thres = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "        # dilate the frame a bit to get some more white area...\n",
    "        # ... makes the detection of contours a bit easier\n",
    "        dilate_frame = cv2.dilate(thres, None, iterations=2)\n",
    "        # append the final result into the `frame_diff_list`\n",
    "        frame_diff_list.append(dilate_frame)\n",
    "        # if we have reached `consecutive_frame` number of frames\n",
    "        if len(frame_diff_list) == consecutive_frame:\n",
    "            # add all the frames in the `frame_diff_list`\n",
    "            sum_frames = sum(frame_diff_list)\n",
    "            # find the contours around the white segmented areas\n",
    "            contours, hierarchy = cv2.findContours(sum_frames, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            # draw the contours, not strictly necessary\n",
    "            for i, cnt in enumerate(contours):\n",
    "                cv2.drawContours(frame, contours, i, (0, 0, 255), 3)\n",
    "            for contour in contours:\n",
    "                # continue through the loop if contour area is less than 500...\n",
    "                # ... helps in removing noise detection\n",
    "                if cv2.contourArea(contour) < 500:\n",
    "                    continue\n",
    "                # get the xmin, ymin, width, and height coordinates from the contours\n",
    "                (x, y, w, h) = cv2.boundingRect(contour)\n",
    "                # draw the bounding boxes\n",
    "                cv2.rectangle(orig_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "            cv2.imshow('Detected Objects', orig_frame)\n",
    "            out.write(orig_frame)\n",
    "            if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_background(file_path):\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    frame_indices = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=50)\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        frames.append(frame)\n",
    "    median_frame = np.median(frames, axis=0).astype(np.uint8)\n",
    "    return median_frame\n",
    "\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    orb = cv2.ORB_create(1000, 1.2)  # ORB detector of 1000 keypoints, scaling pyramid factor=1.2\n",
    "    (kp1, des1) = orb.detectAndCompute(new_image, None)  # Detect keypoints on the new image\n",
    "    (kp2, des2) = orb.detectAndCompute(image_template, None)  # Detect keypoints of the template image\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)  # Matcher\n",
    "    matches = bf.match(des1, des2)  # Extract matches\n",
    "    matches = sorted(matches, key=lambda val: val.distance)  # Sort matches\n",
    "    img2 = cv2.drawKeypoints(image_template, kp2, None, color=(0,255,0), flags=0)\n",
    "    img1 = cv2.drawKeypoints(new_image, kp2, None, color=(0,255,0), flags=0)\n",
    "    plt.imshow(img2)\n",
    "    plt.show()\n",
    "    return len(matches)\n",
    "\n",
    "# Load video file and template image\n",
    "consecutive_frames = 4\n",
    "cap = cv2.VideoCapture(\"HethenHisPelivideo.mp4\")\n",
    "\n",
    "background = get_background(\"HethenHisPelivideo.mp4\")\n",
    "background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "frame_count = 0\n",
    "frame_diff_list = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        frame_count += 1\n",
    "        orig_frame = frame.copy()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_diff = cv2.absdiff(gray, background)\n",
    "        ret, thres = cv2.threshold(frame_diff, 50, 255, cv2.THRESH_BINARY)\n",
    "        dilate_frame = cv2.dilate(thres, None, iterations=2)\n",
    "        frame_diff_list.append(dilate_frame)\n",
    "\n",
    "        if len(frame_diff_list) == consecutive_frames:\n",
    "            sum_frames = sum(frame_diff_list)\n",
    "            contours, hierarchy = cv2.findContours(sum_frames, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) < 500:\n",
    "                    continue\n",
    "                (x, y, w, h) = cv2.boundingRect(contour)\n",
    "                cv2.rectangle(orig_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            # ORB feature detection\n",
    "            kp, des = orb.detectAndCompute(orig_frame, None)\n",
    "            orig_frame = cv2.drawKeypoints(orig_frame, kp, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "            \n",
    "            cv2.imshow('Detected Objects', orig_frame)\n",
    "            if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472bbcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
